{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing parameters\n",
      ".............................................................................................................................\n",
      ".............................................................................................................................\n",
      "Predicting on test set\n",
      ".....................................................................\n",
      ".....................................................................\n",
      "Evaluating\n",
      "Test accuracy: 83.35%\n",
      "Printing test set predictions to predictions.csv\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from math import log, exp\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "# TODO: In your final submission, set `ALPHA`\n",
    "#       to the value you think will be optimal for predicting on\n",
    "#       unseen data\n",
    "# Keep this as a package global variable\n",
    "ALPHA = 19.0\n",
    "# End TODO\n",
    "\n",
    "class IMDBText:\n",
    "    \"\"\"Class to represent text, exposing generator function for words\n",
    "    \"\"\"\n",
    "    def __init__(self, idnum, text):\n",
    "        self.text = text\n",
    "        self.idnum = idnum\n",
    "\n",
    "    def get_words(self):\n",
    "        # Preprocess text\n",
    "        cleaned = bs(self.text, features=\"html.parser\").text\n",
    "        for word in cleaned.strip().split():\n",
    "            word = re.sub(r\"^\\W+\", \"\", word)\n",
    "            word = re.sub(r\"\\W+$\", \"\", word)\n",
    "            if word:\n",
    "                yield word\n",
    "\n",
    "\n",
    "class IMDBReader:\n",
    "    \"\"\"Utility class for reading IMDB data\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        dir_contents = os.listdir(data_dir)\n",
    "        assert \"pos\" in dir_contents and \"neg\" in dir_contents, \\\n",
    "            \"Could not find IMDB data in {}\".format(data_dir)\n",
    "\n",
    "\n",
    "    def get_texts(self, subset):\n",
    "        \"\"\"Generator function over texts in subset ('pos' or 'neg') of data\n",
    "        \"\"\"\n",
    "        assert subset in [\"pos\", \"neg\"], \\\n",
    "            \"Only data subsets 'pos' or 'neg' may be selected\"\n",
    "        for textfile in os.listdir(os.path.join(self.data_dir, subset)):\n",
    "            if textfile[-4:] == \".txt\":\n",
    "                with open(os.path.join(self.data_dir, subset, textfile),\n",
    "                          encoding=\"utf-8\") as f:\n",
    "                    yield IMDBText(textfile[:-4], f.read())\n",
    "\n",
    "\n",
    "class NaiveBayes:\n",
    "    \"\"\"Naive Bayes text categorization model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.train(data)\n",
    "\n",
    "    def train(self, data):\n",
    "        \"\"\"Train model by collecting counts from training corpus\n",
    "        \"\"\"\n",
    "\n",
    "        # Counts of words in positive-/negative-class texts\n",
    "        # w_count[w][y=pos]\n",
    "        self.count_positive = Counter()\n",
    "        # w_count[w][y=neg]\n",
    "        self.count_negative = Counter()\n",
    "\n",
    "        # Total number of reviews for each category\n",
    "        # d_count[y=pos]\n",
    "        self.num_positive_reviews = 0\n",
    "        # d_count[y=neg]\n",
    "        self.num_negative_reviews = 0\n",
    "\n",
    "        # Total number of words in positive/negative reviews\n",
    "        # w_count[y=pos]\n",
    "        self.total_positive_words = 0\n",
    "        # w_count[y=neg]\n",
    "        self.total_negative_words = 0\n",
    "\n",
    "        # Class priors (logprobs)\n",
    "        # log(P(y=pos))\n",
    "        self.p_positive = 0.0\n",
    "        # log(P(y=neg))\n",
    "        self.p_negative = 0.0\n",
    "\n",
    "        # TODO: Iterate through texts and collect count statistics initialized above\n",
    "        #       `self.count_<positive/negative>`\n",
    "        #       `self.num_<positive/negative>_reviews`\n",
    "        #       `self.total_<positive/negative>_words`\n",
    "        for i, text in enumerate(data.get_texts(\"pos\")):\n",
    "            #\n",
    "            # Aggregate counts here\n",
    "            #\n",
    "            pos_words = list(text.get_words())\n",
    "            self.count_positive.update(pos_words)\n",
    "            self.total_positive_words = sum(self.count_positive.values())\n",
    "            self.num_positive_reviews += 1\n",
    "            if i % 100 == 0:\n",
    "                sys.stdout.write(\".\")\n",
    "        print()\n",
    "        for i, text in enumerate(data.get_texts(\"neg\")):\n",
    "            #\n",
    "            # Aggregate counts here\n",
    "            #\n",
    "            neg_words = list(text.get_words())\n",
    "            self.count_negative.update(neg_words)\n",
    "            self.total_negative_words = sum(self.count_negative.values())\n",
    "            self.num_negative_reviews += 1\n",
    "            if i % 100 == 0:\n",
    "                sys.stdout.write(\".\")\n",
    "        print()\n",
    "        # End TODO\n",
    "\n",
    "        # Calculate derived statistics\n",
    "        self.vocab = set(list(self.count_negative.keys())\n",
    "                         + list(self.count_positive.keys()))\n",
    "        self.p_positive = log(float(self.num_positive_reviews)) \\\n",
    "            - log(float(self.num_positive_reviews + self.num_negative_reviews))\n",
    "        self.p_negative = log(float(self.num_negative_reviews)) \\\n",
    "            - log(float(self.num_positive_reviews + self.num_negative_reviews))\n",
    "\n",
    "    def predict(self, data, alpha=1.0):\n",
    "        \"\"\"For each text\n",
    "           - append the text id (file basename) to `text_ids`\n",
    "           - append the predicted label (1.0 or -1.0) to `pred_labels`\n",
    "           - append the correct (gold) label (1.0 or -1.0) to `gold_labels`\n",
    "           - append the probability of the positive (1.0) class to `pred_probs`\n",
    "        \"\"\"\n",
    "        text_ids = []\n",
    "        pred_labels = []\n",
    "        pred_probs = []\n",
    "        gold_labels = []\n",
    "\n",
    "        for classval in [\"pos\", \"neg\"]:\n",
    "            for text in data.get_texts(classval):\n",
    "                text_ids.append(text.idnum)\n",
    "                if classval == \"pos\":\n",
    "                    gold_labels.append(1.0)\n",
    "                else:\n",
    "                    gold_labels.append(-1.0)\n",
    "                if len(text_ids) % 100 == 0:\n",
    "                    sys.stdout.write(\".\")\n",
    "\n",
    "                # TODO: Implement naive Bayes probability estimation to calculate class probabilities\n",
    "                #       and predicted labels for each text in the test set.\n",
    "                #\n",
    "                #       Work using logprobs instead of probabilities in order to avoid numerical underflow.\n",
    "                #       Remember that the model treats multiple occurrences of the same word within a text\n",
    "                #       as independent events\n",
    "\n",
    "                # log(P(Pos|X))\n",
    "                sum_positive = 0\n",
    "                # log(P(Neg|X))\n",
    "                sum_negative = 0\n",
    "                \n",
    "                vocab_alpha = len(self.vocab)*alpha\n",
    "                for word in text.get_words():\n",
    "                    a = self.count_positive[word]+alpha\n",
    "                    b = self.total_positive_words+vocab_alpha\n",
    "                    sum_positive += log(a/b)\n",
    "                    a = self.count_negative[word]+alpha\n",
    "                    b = self.total_negative_words+vocab_alpha\n",
    "                    sum_negative += log(a/b)\n",
    "                sum_positive += self.p_positive\n",
    "                sum_negative += self.p_negative\n",
    "\n",
    "                # End TODO\n",
    "\n",
    "                # Get P(Y|X) by normalizing across log(P(Y,X)) for both values of Y\n",
    "                # 1) Get K = log(P(Pos|X) + P(Neg|X))\n",
    "                normalization_factor = self.log_sum(sum_positive, sum_negative)\n",
    "                # 2) Calculate P(Pos|X) = e^(log(P(Pos,X)) - K)\n",
    "                predicted_prob_positive = exp(sum_positive - normalization_factor)\n",
    "                # 3) Get P(Neg|X) = P(Neg|X) = e^(log(P(Neg,X)) - K)\n",
    "                predicted_prob_negative = 1.0 - predicted_prob_positive\n",
    "\n",
    "                pred_probs.append(predicted_prob_positive)\n",
    "                if predicted_prob_positive > predicted_prob_negative:\n",
    "                    pred_labels.append(1.0)\n",
    "                else:\n",
    "                    pred_labels.append(-1.0)\n",
    "            print()\n",
    "\n",
    "        return text_ids, gold_labels, pred_labels, pred_probs\n",
    "\n",
    "    def log_sum(self, logx, logy):\n",
    "        \"\"\"Utility function to compute $log(exp(logx) + exp(logy))$\n",
    "        while avoiding numerical issues\n",
    "        \"\"\"\n",
    "        m = max(logx, logy)\n",
    "        return m + log(exp(logx - m) + exp(logy - m))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TRAIN_DATA_DIR = \"hw3_data/train\"\n",
    "    TEST_DATA_DIR = \"hw3_data/test\"\n",
    "\n",
    "    print(\"Computing parameters\")\n",
    "    NB = NaiveBayes(IMDBReader(TRAIN_DATA_DIR))\n",
    "\n",
    "    print(\"Predicting on test set\")\n",
    "    TEST_IDS, GOLD_LABELS, PRED_LABELS, PRED_PROBS = NB.predict(IMDBReader(TEST_DATA_DIR), alpha=ALPHA)\n",
    "\n",
    "    print(\"Evaluating\")\n",
    "    ACCURACY = np.sum(np.equal(PRED_LABELS, GOLD_LABELS)) / float(len(GOLD_LABELS))\n",
    "    print(\"Test accuracy: {:.2f}%\".format(100 * ACCURACY))\n",
    "\n",
    "    outfile = \"predictions.csv\"\n",
    "    print(\"Printing test set predictions to {}\".format(outfile))\n",
    "    pd.DataFrame({\"File ID\": TEST_IDS,\n",
    "                  \"Class\": GOLD_LABELS,\n",
    "                  \"Predicted Class\": PRED_LABELS,\n",
    "                  \"Predicted Probability\": PRED_PROBS}).to_csv(outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
