# Natural Language Processing

1. Information-theoretic measures of distributional similarity
   * Entropy
   * Cross entropy
   * Kullback-Leibler divergence
   * Jensen-Shannon divergence
   
2. Text preprocessing using Shell commands

3. Naive Bayes text categorization model

4. Cocke-Younger-Kasami parsing implementation

### HW1 - Information-theoretic measures of distributional similarity
* Entropy
<img src="https://github.com/chandnii7/NLP/blob/main/hw1/images/img1.jpg" height="100" width="300"/>
* Cross entropy
<img src="https://github.com/chandnii7/NLP/blob/main/hw1/images/img2.jpg" height="100" width="300"/>
* Kullback-Leibler divergence
<img src="https://github.com/chandnii7/NLP/blob/main/hw1/images/img3.jpg" height="100" width="300"/>
* Jensen-Shannon divergence
<img src="https://github.com/chandnii7/NLP/blob/main/hw1/images/img4.jpg" height="100" width="400"/>

### HW2 - Text preprocessing using Shell commands
<img src="https://github.com/chandnii7/NLP/blob/main/hw2/images/img1.jpg" height="100" width="400"/>
<img src="https://github.com/chandnii7/NLP/blob/main/hw2/images/img2.jpg" height="200" width="400"/>
<img src="https://github.com/chandnii7/NLP/blob/main/hw2/images/img3.jpg" height="100" width="400"/>

### HW3 - Naive Bayes text categorization model
<img src="https://github.com/chandnii7/NLP/blob/main/hw3/images/img1.jpg" height="200" width="400"/>

### HW4 - Cocke-Younger-Kasami parsing implementation
<img src="https://github.com/chandnii7/NLP/blob/main/hw4/images/img1.jpg" height="300" width="400"/>
