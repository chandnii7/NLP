# Natural Language Processing

1. Information-theoretic measures of distributional similarity
   * Entropy
   * Cross entropy
   * Kullback-Leibler divergence
   * Jensen-Shannon divergence
   
2. Text preprocessing using Shell commands

3. Naive Bayes text categorization model

4. Cocke-Younger-Kasami parsing implementation

### HW1 - Information-theoretic measures of distributional similarity
* Entropy
<img src="https://github.com/chandnii7/NLP/blob/main/images/img1.jpg" height="200" width="200"/>
* Cross entropy
<img src="https://github.com/chandnii7/NLP/blob/main/images/img1.jpg" height="200" width="200"/>
* Kullback-Leibler divergence
<img src="https://github.com/chandnii7/NLP/blob/main/images/img1.jpg" height="200" width="200"/>
* Jensen-Shannon divergence
<img src="https://github.com/chandnii7/NLP/blob/main/images/img1.jpg" height="200" width="200"/>

### HW2 - Text preprocessing using Shell commands
<img src="https://github.com/chandnii7/NLP/blob/main/images/img1.jpg" height="200" width="200"/>
<img src="https://github.com/chandnii7/NLP/blob/main/images/img1.jpg" height="200" width="200"/>
<img src="https://github.com/chandnii7/NLP/blob/main/images/img1.jpg" height="200" width="200"/>

### HW3 - Naive Bayes text categorization model
<img src="https://github.com/chandnii7/NLP/blob/main/images/img1.jpg" height="200" width="200"/>

### HW4 - Cocke-Younger-Kasami parsing implementation
<img src="https://github.com/chandnii7/NLP/blob/main/images/img1.jpg" height="200" width="200"/>
