{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.672\n",
      "2.619\n",
      "2.791\n",
      "2.746\n",
      "0.119\n",
      "0.127\n",
      "0.030\n",
      "0.030\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Calculate information-theoretic measures of distributional\n",
    "similarity based on word frequencies in two texts\n",
    "\"\"\"\n",
    "\n",
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "def read_words(infile):\n",
    "    with open(infile) as input_text:\n",
    "        return [x.strip() for x in input_text.read().split()]\n",
    "\n",
    "\n",
    "def get_counts(word_list):\n",
    "    return collections.Counter(word_list)\n",
    "\n",
    "\n",
    "def create_prob_dist(count_dict):\n",
    "    total_ct = sum(count_dict.values())\n",
    "    p = {x: ct / total_ct for x, ct in count_dict.items()}\n",
    "    return p\n",
    "\n",
    "\n",
    "def count_smoothing(freq_dist, vocabulary, alpha=1):\n",
    "    \"\"\"Implement simple count-based probability smoothing.\n",
    "    Given a target vocabulary and a set of observed count frequencies,\n",
    "    calculate a new set of counts so that Count(x) > 0 for all words\n",
    "    in the target vocabulary.  This is achieved by adding `alpha`\n",
    "    to each observed count\n",
    "    \"\"\"\n",
    "    return {w: freq_dist.get(w, 0) + alpha for w in vocabulary}\n",
    "\n",
    "\n",
    "def entropy(p):\n",
    "    \"\"\"Calculate entropy H(p) for a probability distribution represented\n",
    "    as a mapping (dictionary) from word tokens to probabilities\n",
    "    \"\"\"\n",
    "    h = 0\n",
    "\n",
    "    # TODO -- Calculate entropy value in nats for probability distribution `p`\n",
    "    for x in p:\n",
    "        h -= p[x] * math.log(p[x])\n",
    "\n",
    "    return h\n",
    "\n",
    "\n",
    "def cross_entropy(p1, p2):\n",
    "    \"\"\"Calculate cross-entropy H(p1, p2) for two probability distributions\n",
    "    represented as a mapping (dictionary) from word tokens to\n",
    "    probabilities\n",
    "    \"\"\"\n",
    "    xh = 0\n",
    "\n",
    "    # TODO -- Calculate cross-entropy value H(p1, p2) in nats\n",
    "    for x in p1:\n",
    "        xh -= p1[x] * math.log(p2[x])\n",
    "\n",
    "    return xh\n",
    "\n",
    "\n",
    "def kl_divergence(p1, p2):\n",
    "    \"\"\"Calculate Kullback-Leibler divergence D_{KL}(p1||p2) for two\n",
    "    probability distributions represented as a mapping (dictionary)\n",
    "    from word tokens to probabilities\n",
    "    \"\"\"\n",
    "    kl = 0\n",
    "\n",
    "    # TODO -- Calculate KL divergence D_{KL}(p1||p2) in nats\n",
    "    kl = cross_entropy(p1, p2) - entropy(p1)\n",
    "\n",
    "    return kl\n",
    "\n",
    "\n",
    "def js_divergence(p1, p2):\n",
    "    \"\"\"Calculate Jensen-Shannon divergence D_{JS}(p1||p2) for two\n",
    "    probability distributions represented as a mapping (dictionary)\n",
    "    from word tokens to probabilities\n",
    "    \"\"\"\n",
    "    js = 0\n",
    "\n",
    "    # TODO -- Calculate JS divergence D_{JS}(p1||p2) in nats\n",
    "    m = {k: ((p1.get(k, 0) + p2.get(k, 0))/2.0) for k in p1.keys()}            \n",
    "    js = (kl_divergence(p1, m) + kl_divergence(p2, m))/2\n",
    "    \n",
    "    return js\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"Do not edit this code\n",
    "    \"\"\"\n",
    "    words_a = read_words(\"test_a.txt\")\n",
    "    words_b = read_words(\"test_b.txt\")\n",
    "\n",
    "    ct_a = get_counts(words_a)\n",
    "    ct_b = get_counts(words_b)\n",
    "\n",
    "    vocab = set(ct_a.keys()) | set(ct_b.keys())\n",
    "    ct_a = count_smoothing(ct_a, vocab)\n",
    "    ct_b = count_smoothing(ct_b, vocab)\n",
    "\n",
    "    p_a = create_prob_dist(ct_a)\n",
    "    p_b = create_prob_dist(ct_b)\n",
    "\n",
    "    h_a = entropy(p_a)\n",
    "    h_b = entropy(p_b)\n",
    "    xh_ab = cross_entropy(p_a, p_b)\n",
    "    xh_ba = cross_entropy(p_b, p_a)\n",
    "    kl_ab = kl_divergence(p_a, p_b)\n",
    "    kl_ba = kl_divergence(p_b, p_a)\n",
    "    js_ab = js_divergence(p_a, p_b)\n",
    "    js_ba = js_divergence(p_b, p_a)\n",
    "\n",
    "    for metric in [h_a, h_b, xh_ab, xh_ba,\n",
    "                   kl_ab, kl_ba, js_ab, js_ba]:\n",
    "        print(\"{:.3f}\".format(metric))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
